package org.apache.mahout.clustering.elkan;

import java.io.IOException;
import java.util.Iterator;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.mahout.clustering.Cluster;
import org.apache.mahout.clustering.classify.ClusterClassifier;
import org.apache.mahout.clustering.iterator.ClusterIterator;
import org.apache.mahout.clustering.iterator.ClusterWritable;
import org.apache.mahout.clustering.iterator.ClusteringPolicy;
import org.apache.mahout.clustering.kmeans.KMeansConfigKeys;
import org.apache.mahout.common.ClassUtils;
import org.apache.mahout.common.distance.DistanceMeasure;
import org.apache.mahout.math.DenseVector;
import org.apache.mahout.math.MatrixSlice;
import org.apache.mahout.math.Vector;
import org.apache.mahout.math.VectorWritable;
import org.apache.mahout.math.hadoop.DistributedRowMatrix;
import org.apache.mahout.math.map.OpenHashMap;

public abstract class ElkanMapper<KEYIN, VALUEIN> extends
		Mapper<KEYIN, VALUEIN, IntWritable, ClusterWritable> {

	protected ElkanClassifier classifier;
	protected ClusteringPolicy policy;
	protected Vector medianDistanceClusters;
	protected DistanceMeasure measure;
	protected OpenHashMap<Integer, Vector> clusterDistanceMatrix;
	protected MultipleOutputs<Text, ElkanVectorWritable> m_multiOutputs;
	protected ElkanVectorCache vectorCache;

	@Override
	protected void setup(Context context) throws IOException,
			InterruptedException {
		Configuration conf = context.getConfiguration();
		String measureClass = conf.get(KMeansConfigKeys.DISTANCE_MEASURE_KEY);
		measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);

		String priorClustersPath = conf.get(ClusterIterator.PRIOR_PATH_KEY);
		classifier = new ElkanClassifier();
		classifier.readFromSeqFiles(conf, new Path(priorClustersPath));
		policy = classifier.getPolicy();
		policy.update(classifier);

		int i = 0;
		medianDistanceClusters = new DenseVector(classifier.getModels().size());

		for (Cluster model : classifier.getModels()) {
			Vector centerDistances = new DenseVector(classifier.getModels()
					.size());
			int j = 0;
			for (Cluster oth_model : classifier.getModels()) {
				centerDistances.setQuick(
						j,
						measure.distance(model.getCenter(),
								oth_model.getCenter()));
				j++;
			}

			double sCenter = centerDistances.minValue() / 2;
			medianDistanceClusters.setQuick(i, sCenter);
			i++;
		}

		clusterDistanceMatrix = new OpenHashMap<Integer, Vector>(classifier
				.getModels().size());		

		int rowIndex=0;
		for (Cluster model : classifier.getModels()) {
			Vector distances = new DenseVector(classifier.getModels().size());
			int colIndex = 0;
			for (Cluster oth_model : classifier.getModels()) {
				distances.setQuick(colIndex, measure.distance(oth_model.getCenter(),model.getCenter()));
				colIndex++;
			}			
			clusterDistanceMatrix.put(rowIndex, distances);
			rowIndex++;
		}
		int numClusters=Integer.parseInt(conf.get(ElkanClassifier.NUM_CLUSTERS));
		Path distancesPath=new Path(conf.get(ElkanClassifier.CLUSTER_DISTANCE_KEY));
		vectorCache=new ElkanVectorCache(numClusters,conf,distancesPath);
		
		m_multiOutputs = new MultipleOutputs(context);
	}

	@Override
	protected void cleanup(org.apache.hadoop.mapreduce.Mapper.Context context)
			throws IOException, InterruptedException {
		List<Cluster> clusters = classifier.getModels();
		ClusterWritable cw = new ClusterWritable();
		for (int index = 0; index < clusters.size(); index++) {
			cw.setValue(clusters.get(index));
			context.write(new IntWritable(index), cw);
		}
		m_multiOutputs.close();
		clusterDistanceMatrix.clear();
		clusterDistanceMatrix=null;
		medianDistanceClusters=null;
		
		super.cleanup(context);
	}

}
