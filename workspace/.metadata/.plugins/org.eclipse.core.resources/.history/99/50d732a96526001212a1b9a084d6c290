package org.apache.mahout.clustering.elkan;

import java.io.IOException;
import java.util.Iterator;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.mahout.clustering.Cluster;
import org.apache.mahout.clustering.classify.ClusterClassifier;
import org.apache.mahout.clustering.iterator.ClusterIterator;
import org.apache.mahout.clustering.iterator.ClusterWritable;
import org.apache.mahout.clustering.iterator.ClusteringPolicy;
import org.apache.mahout.clustering.kmeans.KMeansConfigKeys;
import org.apache.mahout.common.ClassUtils;
import org.apache.mahout.common.distance.DistanceMeasure;
import org.apache.mahout.math.DenseVector;
import org.apache.mahout.math.MatrixSlice;
import org.apache.mahout.math.Vector;
import org.apache.mahout.math.VectorWritable;
import org.apache.mahout.math.hadoop.DistributedRowMatrix;
import org.apache.mahout.math.map.OpenHashMap;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class ElkanVectorUpdateMapper
extends
ElkanMapper<WritableComparable<?>, ElkanVectorWritable> {
	private static final Logger log = LoggerFactory
			.getLogger(ElkanVectorUpdateMapper.class);

	private ClusterClassifier classifier;

	private ClusteringPolicy policy;

	private Vector medianDistanceClusters;

	private DistanceMeasure measure;

	private OpenHashMap<Integer, Vector> clusterDistanceMatrix;
	
	private MultipleOutputs<Text, VectorWritable> m_multiOutputs;

	private Vector oldClusterDistances;	

	@Override
	protected void setup(Context context) throws IOException,
			InterruptedException {
		Configuration conf = context.getConfiguration();
		String measureClass = conf.get(KMeansConfigKeys.DISTANCE_MEASURE_KEY);
		measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);

		String priorClustersPath = conf.get(ClusterIterator.PRIOR_PATH_KEY);
		classifier = new ClusterClassifier();
		classifier.readFromSeqFiles(conf, new Path(priorClustersPath));
		policy = classifier.getPolicy();
		policy.update(classifier);

		int i = 0;
		List<Cluster> models = classifier.getModels();
		medianDistanceClusters = new DenseVector(models.size());

		for (Cluster model : models) {
			Vector centerDistances = new DenseVector(models.size());
			int j = 0;
			for (Cluster oth_model : models) {
				centerDistances.setQuick(
						j,
						measure.distance(model.getCenter(),
								oth_model.getCenter()));
				j++;
			}

			double sCenter = centerDistances.minValue() / 2;
			medianDistanceClusters.setQuick(i, sCenter);
			i++;
		}

		clusterDistanceMatrix = new OpenHashMap<Integer, Vector>(models.size());
		Path clusterDistanceMatrixPath = new Path(
				conf.get(ElkanIterator.CENTROID_DISTANCES_PATH_KEY));
		Path clusterDistanceMatrixOutput = new Path(
				conf.get(ElkanIterator.CENTROID_DISTANCES_PATH_KEY) + "/output");
		DistributedRowMatrix rowMatrix = new DistributedRowMatrix(
				clusterDistanceMatrixPath, clusterDistanceMatrixOutput,
				models.size(), models.size());
		rowMatrix.setConf(conf);
		Iterator<MatrixSlice> it = rowMatrix.iterator();
		while (it.hasNext()) {
			MatrixSlice slice = it.next();
			clusterDistanceMatrix.put(slice.index(), slice.vector());
		}

		String newPriorClustersPath = conf
				.get(ElkanIterator.NEW_PRIOR_PATH_KEY);
		ElkanClassifier elkanClassifier = new ElkanClassifier();
		List<Cluster> newModels = elkanClassifier.readNewModelsFromSeqFiles(
				conf, new Path(newPriorClustersPath));
		List<Cluster> oldModels = elkanClassifier.getModels();
		oldClusterDistances = new DenseVector(newModels.size());
		for (i = 0; i < newModels.size(); i++) {
			Vector oldCenter = oldModels.get(i).getCenter();
			Vector newCenter = newModels.get(i).getCenter();
			oldClusterDistances.setQuick(i,
					measure.distance(oldCenter, newCenter));
		}
		
		m_multiOutputs = new MultipleOutputs(context);
		super.setup(context);
	}

	@Override
	protected void map(WritableComparable<?> key, ElkanVectorWritable value,
			Context context) throws IOException, InterruptedException {
		ElkanVector elkanVector = value.get();
		ElkanSteps.updateVectorLimits(elkanVector, oldClusterDistances);
		ElkanSteps.updateVectorCentroid(elkanVector, classifier,
				medianDistanceClusters, clusterDistanceMatrix, measure);
		m_multiOutputs.write("elkanVectors", key, elkanVector);
	}
	
	@Override
	protected void cleanup(
			org.apache.hadoop.mapreduce.Mapper.Context context)
			throws IOException, InterruptedException {
		List<Cluster> clusters = classifier.getModels();
		ClusterWritable cw = new ClusterWritable();
		for (int index = 0; index < clusters.size(); index++) {
			cw.setValue(clusters.get(index));
			context.write(new IntWritable(index), cw);
		}
		super.cleanup(context);
	}
}