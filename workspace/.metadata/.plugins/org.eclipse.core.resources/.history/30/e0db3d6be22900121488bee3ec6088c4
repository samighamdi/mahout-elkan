package org.apache.mahout.clustering.elkan;

import java.io.IOException;
import java.net.URI;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.LinkedHashMap;
import java.util.Map.Entry;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.mahout.clustering.spectral.common.VectorCache;
import org.apache.mahout.common.HadoopUtil;
import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
import org.apache.mahout.math.Vector;
import org.apache.mahout.math.VectorWritable;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.io.Closeables;

/**
 * Follows the same pattern as VectorCache but it keeps many vectors cached in a
 * LRU data structure bounded by a fixed size.
 * 
 * @author gustavo
 * 
 */
public class ElkanVectorCache {

	public static String CACHED_VECTOR_SUFFIX="vector-";
	
	private ArrayList<URI> vectorURIList;
	private LRUVectorCache lruCache;
	private Path cachePath;
	private Configuration conf;
	private boolean overwritePath;
	private boolean deleteOnExit;

	private static final Logger log = LoggerFactory
			.getLogger(VectorCache.class);

	public ElkanVectorCache(int cacheSize, Configuration conf, Path cachePath) {
		this(cacheSize, conf, cachePath, false, false);
	}

	public ElkanVectorCache(int cacheSize, Configuration conf, Path cachePath,
			boolean overwritePath, boolean deleteOnExit) {
		vectorURIList = new ArrayList<URI>();
		lruCache = new LRUVectorCache(cacheSize/2);
		this.conf = conf;
		this.cachePath = cachePath;
		this.overwritePath = overwritePath;
		this.deleteOnExit = deleteOnExit;
	}

	/**
	 * 
	 * @param key
	 *            SequenceFile key
	 * @param vector
	 *            Vector to save, to be wrapped as VectorWritable
	 */
	public void save(IntWritable key, Vector vector) throws IOException {
		Path output = new Path(cachePath, CACHED_VECTOR_SUFFIX+key.get());
		FileSystem fs = FileSystem.get(output.toUri(), conf);
		output = fs.makeQualified(output);
		if (overwritePath) {
			HadoopUtil.delete(conf, output);
		}

		vectorURIList.add(output.toUri());

		SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, output,
				IntWritable.class, VectorWritable.class);
		try {
			writer.append(key, new VectorWritable(vector));
		} finally {
			Closeables.closeQuietly(writer);
		}

		if (deleteOnExit) {
			fs.deleteOnExit(output);
		}
	}
	
	public Vector get(Integer index)
	{
		if (lruCache.containsKey(index))
			return lruCache.get(index);
		else
		{
			Vector vector;
			try {
				vector = loadVector(index);
				lruCache.put(index, vector);
				return vector;
			} catch (IOException e) {
				log.error("Error loading cached vector with ID:"+index);
				return null;
			}
			
		}
	}

	private Vector loadVector(Integer index) throws IOException {
		Path vectorPath = new Path(cachePath, CACHED_VECTOR_SUFFIX+index);
		SequenceFileValueIterator<VectorWritable> iterator =
		        new SequenceFileValueIterator<VectorWritable>(vectorPath, true, conf);
		try {
		      return iterator.next().get();
		    } finally {
		      Closeables.closeQuietly(iterator);
		    }
	}

	
	
}

class LRUVectorCache extends LinkedHashMap<Integer, Vector> {
	private int maxEntries;

	public LRUVectorCache() {
		this(0);
	}

	public LRUVectorCache(int maxEntries) {
		super(maxEntries + 1, 1.0f, true);
		this.maxEntries = maxEntries;
	}

	@Override
	protected boolean removeEldestEntry(Entry<Integer, Vector> eldest) {
		return super.size() > maxEntries;
	}

}