package org.apache.mahout.clustering.elkan;

import java.io.IOException;
import java.net.URI;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.LinkedHashMap;
import java.util.Map.Entry;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.mahout.clustering.spectral.common.VectorCache;
import org.apache.mahout.common.HadoopUtil;
import org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator;
import org.apache.mahout.math.Vector;
import org.apache.mahout.math.VectorWritable;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.io.Closeables;

/**
 * Follows the same pattern as VectorCache but it keeps many vectors cached in a
 * LRU data structure bounded by a fixed size.
 * 
 * @author gustavo
 * 
 */
public class ElkanVectorCache {

	public static String CACHED_VECTOR_SUFFIX="-index";
	
	private ArrayList<URI> vectorURIList;
	private LRUVectorCache lruCache;
	private Path cachePath;
	private Configuration conf;
	private boolean overwritePath;
	private boolean deleteOnExit;

	private static final Logger log = LoggerFactory
			.getLogger(VectorCache.class);

	public ElkanVectorCache(Configuration conf, Path cachePath,
			boolean overwritePath, boolean deleteOnExit) {
		this(0, conf, cachePath, overwritePath, deleteOnExit);
	}

	public ElkanVectorCache(int cacheSize, Configuration conf, Path cachePath,
			boolean overwritePath, boolean deleteOnExit) {
		vectorURIList = new ArrayList<URI>();
		lruCache = new LRUVectorCache(cacheSize);
		this.conf = conf;
		this.cachePath = cachePath;
		this.overwritePath = overwritePath;
		this.deleteOnExit = deleteOnExit;
	}

	/**
	 * 
	 * @param key
	 *            SequenceFile key
	 * @param vector
	 *            Vector to save, to be wrapped as VectorWritable
	 */
	public void save(IntWritable key, Vector vector) throws IOException {
		Path output = new Path(cachePath, CACHED_VECTOR_SUFFIX+key.get());
		FileSystem fs = FileSystem.get(output.toUri(), conf);
		output = fs.makeQualified(output);
		if (overwritePath) {
			HadoopUtil.delete(conf, output);
		}

		vectorURIList.add(output.toUri());

		SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, output,
				IntWritable.class, VectorWritable.class);
		try {
			writer.append(key, new VectorWritable(vector));
		} finally {
			Closeables.closeQuietly(writer);
		}

		if (deleteOnExit) {
			fs.deleteOnExit(output);
		}
	}

	public void close(boolean distribute) {
		if (distribute && vectorURIList.size()>0) {
			for (URI fileURI : vectorURIList) {
				DistributedCache.addCacheFile(fileURI, conf);
			}
		}
	}
	
	public Vector get(Integer index)
	{
		if (lruCache.containsKey(index))
			return lruCache.get(index);
		else
		{
			Vector vector=loadVector(index);
			lruCache.put(index, vector);
			return vector;
		}
	}

	private Vector loadVector(Integer index) {
		Path vectorPath = new Path(cachePath, CACHED_VECTOR_SUFFIX+index);
		SequenceFileValueIterator<VectorWritable> iterator =
		        new SequenceFileValueIterator<VectorWritable>(vectorPath, true, conf);
		return null;
	}

	/**
	 * Loads the vector from {@link DistributedCache}. Returns null if no vector
	 * exists.
	 */
	public void load() throws IOException {
		URI[] files = DistributedCache.getCacheFiles(conf);
		if (files == null || files.length < 1) {
			throw new RuntimeException("No vectors file index found");
		}
		log.info("Vector Cache Index Files are: {}", Arrays.toString(files));
		Text vectorIndex=null;
		SequenceFile.Reader<IntWritable,VectorWritable> read=new SequenceFile.Reader<IntWritable,VectorWritable>(); 
		SequenceFileValueIterator<Text> iterator = new SequenceFileValueIterator<Text>(
				cachePath, true, conf);
		try {
			vectorIndex=iterator.next().
		} finally {
			Closeables.closeQuietly(iterator);
		}
	}
	
}

class LRUVectorCache extends LinkedHashMap<Integer, Vector> {
	private int maxEntries;

	public LRUVectorCache() {
		this(0);
	}

	public LRUVectorCache(int maxEntries) {
		super(maxEntries + 1, 1.0f, true);
		this.maxEntries = maxEntries;
	}

	@Override
	protected boolean removeEldestEntry(Entry<Integer, Vector> eldest) {
		return super.size() > maxEntries;
	}

}